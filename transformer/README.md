# Handcrafted Transformer

This project is a handcrafted implementation of the Transformer architecture.

## Articles Used
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008). [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint [arXiv:1810.04805](https://arxiv.org/abs/1810.04805).

## Transformer Architecture

![Transformer Architecture](https://upload.wikimedia.org/wikipedia/commons/3/34/Transformer%2C_full_architecture.png)

The Transformer model architecture includes the following components:
- **Encoder**: Processes the input sequence.
- **Decoder**: Generates the output sequence.
- **Attention Mechanism**: Allows the model to focus on different parts of the input sequence.

For more details, please refer to the articles mentioned above.
