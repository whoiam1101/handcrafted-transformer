# ðŸ¤– Handcrafted Transformer

This project is a handcrafted implementation of the Transformer architecture, inspired by one of the most impactful advancements in deep learning.  

## ðŸ“š Articles Referenced  

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). *Attention Is All You Need*. In *Advances in Neural Information Processing Systems*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762).  

## ðŸ§  Transformer Architecture  

![Transformer Architecture](https://upload.wikimedia.org/wikipedia/commons/3/34/Transformer%2C_full_architecture.png)  

The Transformer model consists of the following key components:  

- **Encoder:** Processes the input sequence and encodes contextual information.  
- **Decoder:** Generates the output sequence while attending to encoder outputs.  
- **Attention Mechanism:** Focuses on different parts of the input sequence, enabling effective context-aware processing.  

This project offers a handcrafted exploration of the intricate components of the Transformer model, built from scratch for a deeper understanding.  

For more insights, check out the referenced paper linked above.  
