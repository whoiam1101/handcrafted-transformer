{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Transformer & Trainer\n",
    "\n",
    "In this notebook, I will test my implementation of Transformer and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.lion.lion import Lion\n",
    "from src.transformer.transformer import Transformer\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "dataset_url = \"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\"\n",
    "data_path = \"data/simplebooks.zip\"\n",
    "extracted_path = \"data/simplebooks/\"\n",
    "\n",
    "if not os.path.exists(extracted_path):\n",
    "    urllib.request.urlretrieve(dataset_url, data_path)\n",
    "\n",
    "    with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data/\")\n",
    "    \n",
    "    os.remove(data_path)\n",
    "else:\n",
    "    print(\"Dataset already downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f2cced23b4cceaeeb84fd3023f5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/114695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc74b36ff604429db89173a5e36ab217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/13383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186f35621c1473586975a1710ca3b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_path = os.path.join(extracted_path, 'simplebooks-2/train.txt')\n",
    "valid_path = os.path.join(extracted_path, 'simplebooks-2/valid.txt')\n",
    "test_path = os.path.join(extracted_path, 'simplebooks-2/test.txt')\n",
    "\n",
    "datasets = load_dataset('text', data_files={\n",
    "    'train': train_path,\n",
    "    'validation': valid_path,\n",
    "    'test': test_path\n",
    "})\n",
    "\n",
    "datasets = datasets.filter(lambda example: example[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 55366\n",
      "}) \n",
      " \" Find the man who stole the pearls , \" said the king . Away went the guards looking high and low for the thief .\n"
     ]
    }
   ],
   "source": [
    "print(datasets['train'], '\\n', datasets['train']['text'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71964962ed264e47aca2568a08a3f87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/55366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c713c12dce884c6888f9943b94a55d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf289fc7ecc4cae8d4793ae0ac877cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"].with_format(\"torch\"),\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"test\"].with_format(\"torch\")[13]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return super().__getitem__(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [\"Bunny Rabbit 's <unk>\", 'By', 'Mary <unk> <unk>', \"Bunny Rabbit 's <unk>\", 'Mr. and Mrs. Rabbit and the three little rabbits lived in the woods .', 'Each little rabbit had a name .', 'There was Bunny Rabbit , <unk> Rabbit , and Billy Rabbit .', 'Bunny was full of fun , and liked to play tricks on his brothers .'], 'input_ids': tensor([[ 6100,    29,    63,  ...,     0,     0,     0],\n",
      "        [  938,     1,     0,  ...,     0,     0,     0],\n",
      "        [ 3790,     2,     2,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1698,   385, 18383,  ...,     0,     0,     0],\n",
      "        [  290,    47,  6100,  ...,     0,     0,     0],\n",
      "        [ 6100,    29,    63,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.metrics import compute_metrics\n",
    "\n",
    "tiny_model = Transformer(\n",
    "    num_layers=8,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    d_ff=128,\n",
    "    input_dim=512,\n",
    "    output_dim=tokenizer.vocab_size,\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=tiny_model,\n",
    "    num_epochs=1000,\n",
    "    batch_size=8,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    loss_fn=F.cross_entropy,\n",
    "    metrics_fn=compute_metrics,\n",
    "    optimizer=Lion,\n",
    "    optimizer_kwargs=dict(lr=1e-4),\n",
    "    checkpoint_path='./'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/6921 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/PetProjects/handcrafted-transformer/src/training/trainer.py:85\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader,\n\u001b[1;32m     82\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[0;32m---> 85\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     87\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     88\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
