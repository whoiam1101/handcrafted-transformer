


import os
import sys

module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)

import torch
import torch.nn as nn
import torch.nn.functional as F

from src.lion.lion import Lion
from src.transformer.transformer import Transformer
from src.training.trainer import Trainer

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


import zipfile
import urllib.request
import os

dataset_url = "https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip"
data_path = "data/simplebooks.zip"
extracted_path = "data/simplebooks/"

if not os.path.exists(extracted_path):
    urllib.request.urlretrieve(dataset_url, data_path)

    with zipfile.ZipFile(data_path, 'r') as zip_ref:
        zip_ref.extractall("data/")
    
    os.remove(data_path)
else:
    print("Dataset already downloaded and extracted.")


from datasets import load_dataset

train_path = os.path.join(extracted_path, 'simplebooks-2/train.txt')
valid_path = os.path.join(extracted_path, 'simplebooks-2/valid.txt')
test_path = os.path.join(extracted_path, 'simplebooks-2/test.txt')

datasets = load_dataset('text', data_files={
    'train': train_path,
    'validation': valid_path,
    'test': test_path
})

datasets = datasets.filter(lambda example: example["text"])


print(datasets['train'], '\n', datasets['train']['text'][14])


from transformers import T5Tokenizer
from torch.utils.data import DataLoader, Dataset


tokenizer = T5Tokenizer.from_pretrained("t5-small")

def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512,
    )

tokenized_datasets = datasets.map(preprocess_function, batched=True)

test_dataloader = DataLoader(
    tokenized_datasets["test"].with_format("torch"),
    batch_size=8,
)


len(tokenized_datasets["test"].with_format("torch")[13]['text'])


class TextDataset(Dataset):
    def __init__(self):
        super().__init__()

    def __getitem__(self, index):
        return super().__getitem__(index)

    def __len__(self):
        pass



for batch in test_dataloader:
    print(batch)
    break


tokenizer.vocab_size


from src.training.metrics import compute_metrics

tiny_model = Transformer(
    num_layers=8,
    d_model=128,
    num_heads=8,
    d_ff=128,
    input_dim=512,
    output_dim=tokenizer.vocab_size,
    max_len=512
)

trainer = Trainer(
    model=tiny_model,
    num_epochs=1000,
    batch_size=8,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['validation'],
    loss_fn=F.cross_entropy,
    metrics_fn=compute_metrics,
    optimizer=Lion,
    optimizer_kwargs=dict(lr=1e-4),
    checkpoint_path='./'
)


trainer.train()
